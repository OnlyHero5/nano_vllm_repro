"""Day 2 æµ‹è¯•è„šæœ¬ - éªŒè¯æ¨¡å‹æ¶æ„

æ³¨æ„ï¼šnano-vllm æ˜¯æ¨ç†æ¡†æ¶ï¼Œæ‰€æœ‰æµ‹è¯•éƒ½åœ¨ inference_mode ä¸‹è¿è¡Œ
"""

import sys
sys.path.insert(0, '.')

import torch
from layers.layernorm import RMSNorm
from layers.activation import SiluAndMul
from layers.rotary_embedding import RotaryEmbedding, apply_rotary_emb, get_rope


@torch.inference_mode()  # æ¨ç†æ¨¡å¼ï¼šç¦ç”¨æ¢¯åº¦ï¼Œå…è®¸åŸåœ°æ“ä½œ
def test_rmsnorm():
    """æµ‹è¯• RMSNorm"""
    print("=" * 50)
    print("æµ‹è¯• RMSNorm")
    print("=" * 50)
    
    hidden_size = 128
    batch_size = 2
    seq_len = 10
    
    norm = RMSNorm(hidden_size)
    
    # æµ‹è¯•åŸºç¡€ forward
    x = torch.randn(batch_size, seq_len, hidden_size)
    out = norm(x)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {out.shape}")
    assert out.shape == x.shape
    
    # éªŒè¯å½’ä¸€åŒ–æ•ˆæœï¼šè¾“å‡ºçš„ RMS åº”è¯¥æ¥è¿‘ 1
    rms = out.pow(2).mean(dim=-1).sqrt()
    print(f"è¾“å‡º RMS å‡å€¼: {rms.mean().item():.4f} (åº”æ¥è¿‘ weight åˆå§‹å€¼ 1.0)")
    
    # æµ‹è¯•èåˆæ®‹å·®ç‰ˆæœ¬
    x2 = torch.randn(batch_size, seq_len, hidden_size)
    residual = torch.randn_like(x2)
    out2, new_residual = norm(x2, residual)
    print(f"èåˆæ®‹å·®ç‰ˆæœ¬ - è¾“å‡ºå½¢çŠ¶: {out2.shape}, æ–°æ®‹å·®å½¢çŠ¶: {new_residual.shape}")
    
    print("âœ… RMSNorm æµ‹è¯•é€šè¿‡!\n")


@torch.inference_mode()
def test_silu_and_mul():
    """æµ‹è¯• SiluAndMul (SwiGLU)"""
    print("=" * 50)
    print("æµ‹è¯• SiluAndMul")
    print("=" * 50)
    
    act = SiluAndMul()
    
    # è¾“å…¥æ˜¯ gate å’Œ up æ‹¼æ¥çš„ç»“æœ
    batch_size = 2
    seq_len = 10
    intermediate_size = 64
    
    x = torch.randn(batch_size, seq_len, intermediate_size * 2)
    out = act(x)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape} (2 * intermediate_size)")
    print(f"è¾“å‡ºå½¢çŠ¶: {out.shape} (intermediate_size)")
    assert out.shape == (batch_size, seq_len, intermediate_size)
    
    # æ‰‹åŠ¨éªŒè¯è®¡ç®—
    x2 = torch.randn(batch_size, seq_len, intermediate_size * 2)
    gate, up = x2.chunk(2, dim=-1)
    expected = torch.nn.functional.silu(gate) * up
    out2 = act(x2)
    assert torch.allclose(out2, expected, atol=1e-6)
    
    print("âœ… SiluAndMul æµ‹è¯•é€šè¿‡!\n")


@torch.inference_mode()
def test_rope():
    """æµ‹è¯• RoPE"""
    print("=" * 50)
    print("æµ‹è¯• RoPE (æ—‹è½¬ä½ç½®ç¼–ç )")
    print("=" * 50)
    
    head_dim = 64
    max_position = 1024
    base = 10000.0
    
    rope = get_rope(head_dim, head_dim, max_position, base)
    
    # æµ‹è¯•æ•°æ®
    num_tokens = 5
    num_heads = 4
    num_kv_heads = 2
    
    positions = torch.arange(num_tokens)
    query = torch.randn(num_tokens, num_heads, head_dim)
    key = torch.randn(num_tokens, num_kv_heads, head_dim)
    
    # ä¿å­˜åŸå§‹æ¨¡é•¿ç”¨äºæ¯”è¾ƒ
    q_norm_before = query.norm(dim=-1).clone()
    
    q_rot, k_rot = rope(positions, query, key)
    
    print(f"ä½ç½®ç´¢å¼•: {positions}")
    print(f"Query å½¢çŠ¶: {query.shape} â†’ {q_rot.shape}")
    print(f"Key å½¢çŠ¶: {key.shape} â†’ {k_rot.shape}")
    
    # éªŒè¯ï¼šæ—‹è½¬ä¸æ”¹å˜å‘é‡çš„æ¨¡é•¿
    q_norm_after = q_rot.norm(dim=-1)
    print(f"Query æ¨¡é•¿å˜åŒ–: {(q_norm_after / q_norm_before).mean().item():.4f} (åº”æ¥è¿‘ 1.0)")
    
    print("âœ… RoPE æµ‹è¯•é€šè¿‡!\n")


@torch.inference_mode()
def test_rope_relative_position():
    """éªŒè¯ RoPE çš„ç›¸å¯¹ä½ç½®ç¼–ç æ€§è´¨"""
    print("=" * 50)
    print("éªŒè¯ RoPE ç›¸å¯¹ä½ç½®æ€§è´¨")
    print("=" * 50)
    
    head_dim = 64
    
    # æ¯æ¬¡æµ‹è¯•åˆ›å»ºæ–°çš„ rope å®ä¾‹ï¼Œé¿å…ç¼“å­˜é—®é¢˜
    rope = RotaryEmbedding(head_dim, head_dim, 1024, 10000.0)
    
    # åˆ›å»ºä¸¤ä¸ªç›¸åŒçš„å‘é‡
    q_original = torch.randn(1, 1, head_dim)
    k_original = q_original.clone()
    
    # æ”¾åœ¨ä¸åŒä½ç½®
    pos1 = torch.tensor([0])
    pos2 = torch.tensor([5])
    pos3 = torch.tensor([10])
    
    # ç›¸åŒä½ç½®
    q1, k1 = rope(pos1, q_original.clone(), k_original.clone())
    dot_same = (q1 * k1).sum()
    
    # ç›¸å·® 5 çš„ä½ç½® (0 å’Œ 5)
    q_at_0, _ = rope(pos1, q_original.clone(), k_original.clone())
    _, k_at_5 = rope(pos2, q_original.clone(), k_original.clone())
    dot_diff_5 = (q_at_0 * k_at_5).sum()
    
    # ç›¸å·® 5 çš„ä½ç½® (5 å’Œ 10)
    q_at_5, _ = rope(pos2, q_original.clone(), k_original.clone())
    _, k_at_10 = rope(pos3, q_original.clone(), k_original.clone())
    dot_diff_5_v2 = (q_at_5 * k_at_10).sum()
    
    print(f"ç›¸åŒä½ç½®çš„ç‚¹ç§¯: {dot_same.item():.4f}")
    print(f"ä½ç½® 0 å’Œ 5 çš„ç‚¹ç§¯: {dot_diff_5.item():.4f}")
    print(f"ä½ç½® 5 å’Œ 10 çš„ç‚¹ç§¯: {dot_diff_5_v2.item():.4f}")
    print(f"ç›¸å¯¹ä½ç½®ç›¸åŒæ—¶ç‚¹ç§¯å·®å¼‚: {abs(dot_diff_5.item() - dot_diff_5_v2.item()):.6f}")
    
    # éªŒè¯ï¼šç›¸åŒç›¸å¯¹ä½ç½®çš„ç‚¹ç§¯åº”è¯¥ç›¸ç­‰
    assert abs(dot_diff_5.item() - dot_diff_5_v2.item()) < 1e-4, "ç›¸å¯¹ä½ç½®ç¼–ç æ€§è´¨ä¸æ»¡è¶³!"
    
    print("âœ… RoPE ç›¸å¯¹ä½ç½®æ€§è´¨éªŒè¯é€šè¿‡!\n")


@torch.inference_mode()
def test_qwen3_model():
    """æµ‹è¯• Qwen3 æ¨¡å‹"""
    print("=" * 50)
    print("æµ‹è¯• Qwen3 æ¨¡å‹")
    print("=" * 50)
    
    from models.qwen3 import Qwen3ForCausalLM
    from dataclasses import dataclass
    
    # åˆ›å»ºä¸€ä¸ªå°å‹é…ç½®ç”¨äºæµ‹è¯•
    @dataclass
    class TestConfig:
        vocab_size: int = 1000
        hidden_size: int = 128
        num_hidden_layers: int = 2
        num_attention_heads: int = 4
        num_key_value_heads: int = 2
        intermediate_size: int = 256
        max_position_embeddings: int = 512
        rms_norm_eps: float = 1e-6
        attention_bias: bool = False  # Qwen3 é»˜è®¤ False
        rope_theta: float = 10000.0
        tie_word_embeddings: bool = False
    
    config = TestConfig()
    model = Qwen3ForCausalLM(config)
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    
    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    num_tokens = 10
    input_ids = torch.randint(0, config.vocab_size, (num_tokens,))
    
    logits = model(input_ids)
    
    print(f"è¾“å…¥ token æ•°: {num_tokens}")
    print(f"è¾“å‡º logits å½¢çŠ¶: {logits.shape}")
    assert logits.shape == (num_tokens, config.vocab_size)
    
    # æµ‹è¯•è‡ªå›å½’ç”Ÿæˆï¼ˆç®€å•æ¨¡æ‹Ÿï¼‰
    print("\næ¨¡æ‹Ÿè‡ªå›å½’ç”Ÿæˆ:")
    generated = input_ids.tolist()
    for _ in range(3):
        logits = model(torch.tensor(generated))
        next_token = logits[-1].argmax().item()
        generated.append(next_token)
        print(f"  ç”Ÿæˆ token: {next_token}")
    
    print("âœ… Qwen3 æ¨¡å‹æµ‹è¯•é€šè¿‡!\n")


@torch.inference_mode()
def test_gqa():
    """æµ‹è¯• Grouped Query Attention"""
    print("=" * 50)
    print("æµ‹è¯• GQA (Grouped Query Attention)")
    print("=" * 50)
    
    from models.qwen3 import Qwen3Attention
    
    hidden_size = 128
    num_heads = 8
    num_kv_heads = 2  # GQA: æ¯ 4 ä¸ª Q head å…±äº« 1 ä¸ª KV head
    
    attn = Qwen3Attention(
        hidden_size=hidden_size,
        num_heads=num_heads,
        num_kv_heads=num_kv_heads,
        qkv_bias=False,  # Qwen3 é»˜è®¤ Falseï¼Œä¼šä½¿ç”¨ QK Norm
    )
    attn.eval()
    
    num_tokens = 5
    hidden_states = torch.randn(num_tokens, hidden_size)
    positions = torch.arange(num_tokens)
    
    output = attn(positions, hidden_states, attention_mask=None)
    
    print(f"num_heads: {num_heads}, num_kv_heads: {num_kv_heads}")
    print(f"æ¯ä¸ª KV head è¢« {num_heads // num_kv_heads} ä¸ª Q head å…±äº«")
    print(f"è¾“å…¥å½¢çŠ¶: {hidden_states.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    
    assert output.shape == hidden_states.shape
    print("âœ… GQA æµ‹è¯•é€šè¿‡!\n")


if __name__ == "__main__":
    print("=" * 50)
    print("nano-vllm Day 2 æµ‹è¯• (æ¨ç†æ¨¡å¼)")
    print("=" * 50)
    print()
    
    test_rmsnorm()
    test_silu_and_mul()
    test_rope()
    test_rope_relative_position()
    test_qwen3_model()
    test_gqa()
    
    print("=" * 50)
    print("ğŸ‰ Day 2 æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    print("=" * 50)