"""éªŒè¯åŸºç¡€æ•°æ®ç»“æ„"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from config import Config
from sampling_params import SamplingParams

from engine.sequence import Sequence, SequenceStatus
from utils.context import get_context, set_context, reset_context
import torch



def test_sampling_params():
    """æµ‹è¯• SamplingParams"""
    print("=" * 50)
    print("æµ‹è¯• SamplingParams")
    print("=" * 50)
    
    # é»˜è®¤å‚æ•°
    sp = SamplingParams()
    print(f"é»˜è®¤å‚æ•°: temperature={sp.temperature}, max_tokens={sp.max_tokens}")
    assert sp.temperature == 1.0
    assert sp.max_tokens == 4096
    assert sp.ignore_eos == False
    
    # è‡ªå®šä¹‰å‚æ•°
    sp2 = SamplingParams(temperature=0.7, max_tokens=128, ignore_eos=True)
    print(f"è‡ªå®šä¹‰å‚æ•°: temperature={sp2.temperature}, max_tokens={sp2.max_tokens}")
    
    # æµ‹è¯•å‚æ•°æ ¡éªŒ
    try:
        SamplingParams(temperature=0)  # åº”è¯¥å¤±è´¥
        print("âŒ åº”è¯¥æŠ›å‡ºå¼‚å¸¸ä½†æ²¡æœ‰")
    except AssertionError as e:
        print(f"âœ… æ­£ç¡®æ‹’ç» temperature=0: {e}")
    
    print("âœ… SamplingParams æµ‹è¯•é€šè¿‡!\n")


def test_sequence():
    """æµ‹è¯• Sequence"""
    print("=" * 50)
    print("æµ‹è¯• Sequence")
    print("=" * 50)
    
    # æ¨¡æ‹Ÿ prompt tokens
    prompt_tokens = [15496, 11, 703, 527, 499, 30]  # "Hello, how are you?"
    
    # åˆ›å»ºåºåˆ—
    seq = Sequence(prompt_tokens, SamplingParams(temperature=0.8, max_tokens=100))
    
    print(f"seq_id: {seq.seq_id}")
    print(f"status: {seq.status}")
    print(f"num_tokens: {seq.num_tokens}")
    print(f"num_prompt_tokens: {seq.num_prompt_tokens}")
    print(f"num_completion_tokens: {seq.num_completion_tokens}")
    print(f"temperature: {seq.temperature}")
    
    # éªŒè¯åˆå§‹çŠ¶æ€
    assert seq.status == SequenceStatus.WAITING
    assert len(seq) == 6
    assert seq.num_completion_tokens == 0
    assert seq.is_finished == False
    
    # æµ‹è¯• block è®¡ç®—
    print(f"\nBlock ç›¸å…³å±æ€§:")
    print(f"  block_size: {seq.block_size}")
    print(f"  num_blocks: {seq.num_blocks}")  # ceil(6/256) = 1
    print(f"  last_block_num_tokens: {seq.last_block_num_tokens}")
    
    # æ¨¡æ‹Ÿç”Ÿæˆè¿‡ç¨‹
    print(f"\næ¨¡æ‹Ÿ Decode è¿‡ç¨‹:")
    seq.status = SequenceStatus.RUNNING
    seq.block_table = [0]  # å‡è®¾åˆ†é…äº†ç‰©ç†å— 0
    
    # ç”Ÿæˆ 3 ä¸ª token
    generated_tokens = [40, 716, 7024]  # "I", "am", "fine"
    for token in generated_tokens:
        seq.append_token(token)
        print(f"  ç”Ÿæˆ token {token}, å½“å‰é•¿åº¦: {len(seq)}")
    
    assert seq.num_tokens == 9
    assert seq.num_completion_tokens == 3
    assert seq.last_token == 7024
    assert seq.completion_token_ids == generated_tokens
    
    # æµ‹è¯•å®ŒæˆçŠ¶æ€
    seq.status = SequenceStatus.FINISHED
    assert seq.is_finished == True
    
    print(f"\næœ€ç»ˆçŠ¶æ€:")
    print(f"  status: {seq.status}")
    print(f"  token_ids: {seq.token_ids}")
    print(f"  prompt_token_ids: {seq.prompt_token_ids}")
    print(f"  completion_token_ids: {seq.completion_token_ids}")
    
    print("âœ… Sequence æµ‹è¯•é€šè¿‡!\n")


def test_context():
    """æµ‹è¯• Context"""
    print("=" * 50)
    print("æµ‹è¯• Context")
    print("=" * 50)
    
    # åˆå§‹çŠ¶æ€
    ctx = get_context()
    print(f"åˆå§‹çŠ¶æ€: is_prefill={ctx.is_prefill}")
    assert ctx.is_prefill == False
    
    # æ¨¡æ‹Ÿ Prefill é˜¶æ®µè®¾ç½®
    set_context(
        is_prefill=True,
        cu_seqlens_q=torch.tensor([0, 4, 6, 11], dtype=torch.int32),
        cu_seqlens_k=torch.tensor([0, 4, 6, 11], dtype=torch.int32),
        max_seqlen_q=5,
        max_seqlen_k=5,
        slot_mapping=torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
    )
    
    ctx = get_context()
    print(f"Prefill é˜¶æ®µ:")
    print(f"  is_prefill: {ctx.is_prefill}")
    print(f"  cu_seqlens_q: {ctx.cu_seqlens_q}")
    print(f"  max_seqlen_q: {ctx.max_seqlen_q}")
    assert ctx.is_prefill == True
    
    # æ¨¡æ‹Ÿ Decode é˜¶æ®µè®¾ç½®
    set_context(
        is_prefill=False,
        context_lens=torch.tensor([10, 8, 15]),
        block_tables=torch.tensor([[0, 1], [2, 3], [4, 5]])
    )
    
    ctx = get_context()
    print(f"\nDecode é˜¶æ®µ:")
    print(f"  is_prefill: {ctx.is_prefill}")
    print(f"  context_lens: {ctx.context_lens}")
    print(f"  block_tables shape: {ctx.block_tables.shape}")
    assert ctx.is_prefill == False
    
    # é‡ç½®
    reset_context()
    ctx = get_context()
    assert ctx.is_prefill == False
    assert ctx.cu_seqlens_q is None
    
    print("âœ… Context æµ‹è¯•é€šè¿‡!\n")


def test_config():
    """æµ‹è¯• Configï¼ˆéœ€è¦æœ‰æ•ˆçš„æ¨¡å‹è·¯å¾„ï¼‰"""
    print("=" * 50)
    print("æµ‹è¯• Config (è·³è¿‡ï¼Œéœ€è¦æ¨¡å‹è·¯å¾„)")
    print("=" * 50)
    
    # å¦‚æœä½ æœ‰æ¨¡å‹ï¼Œå¯ä»¥å–æ¶ˆæ³¨é‡Šä¸‹é¢çš„ä»£ç 
    # config = Config(model="/path/to/your/Qwen3-0.6B")
    # print(f"æ¨¡å‹é…ç½®: {config.hf_config}")
    # print(f"max_model_len: {config.max_model_len}")
    # print(f"kvcache_block_size: {config.kvcache_block_size}")
    
    print("â­ï¸ Config æµ‹è¯•è·³è¿‡ï¼ˆéœ€è¦æœ‰æ•ˆæ¨¡å‹è·¯å¾„ï¼‰\n")


if __name__ == "__main__":
    test_sampling_params()
    test_sequence()
    test_context()
    test_config()
    
    print("=" * 50)
    print("ğŸ‰ Day 1 æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    print("=" * 50)