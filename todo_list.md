# nano-vLLM å¤ç°é¡¹ç›® - 7å¤©å†²åˆºå¾…åŠæ¸…å•

## ğŸ“‹ é¡¹ç›®æ¦‚è§ˆ

- **é¡¹ç›®ç›®æ ‡**ï¼šä»é›¶å¤ç° nano-vllmï¼Œæ·±å…¥ç†è§£ vLLM æ ¸å¿ƒæ¶æ„
- **å‚è€ƒä»“åº“**ï¼š<https://github.com/GeeeekExplorer/nano-vllm.git>
- **æ—¶é—´å‘¨æœŸ**ï¼š7 å¤©ï¼ˆ2026å¹´1æœˆ2æ—¥ - 2026å¹´1æœˆ8æ—¥ï¼‰
- **æ ¸å¿ƒæŠ€æœ¯ç‚¹**ï¼šPagedAttentionã€KV Cache ç®¡ç†ã€FlashAttentionã€CUDA Graphã€Tensor Parallelism

---

## ğŸ¯ æ ¸å¿ƒå­¦ä¹ ç›®æ ‡

1. **ç†è§£ vLLM æ¶æ„**ï¼šPagedAttention å’Œ KV Cache ç®¡ç†æœºåˆ¶
2. **æŒæ¡é«˜æ€§èƒ½æ¨ç†**ï¼šFlashAttention é›†æˆã€CUDA Graph ä¼˜åŒ–
3. **åˆ†å¸ƒå¼ç³»ç»Ÿ**ï¼šåŸºäºå¤šè¿›ç¨‹çš„ Tensor Parallelism (TP)

---

## ğŸ“ é¡¹ç›®ç›®å½•ç»“æ„ï¼ˆç›®æ ‡ï¼‰

```
nanovllm/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ config.py
â”œâ”€â”€ sampling_params.py
â”œâ”€â”€ llm.py
â”œâ”€â”€ engine/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ sequence.py
â”‚   â”œâ”€â”€ block_manager.py
â”‚   â”œâ”€â”€ scheduler.py
â”‚   â”œâ”€â”€ model_runner.py
â”‚   â””â”€â”€ llm_engine.py
â”œâ”€â”€ layers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ layernorm.py
â”‚   â”œâ”€â”€ activation.py
â”‚   â”œâ”€â”€ rotary_embedding.py
â”‚   â”œâ”€â”€ attention.py
â”‚   â”œâ”€â”€ linear.py
â”‚   â””â”€â”€ sampler.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ qwen3.py
â””â”€â”€ utils/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ context.py
```

---

## ğŸ“… æ¯æ—¥ä»»åŠ¡æ¸…å•

---

### Day 1: åŸºç¡€è®¾æ–½ä¸æ•°æ®ç»“æ„ (Infrastructure & Data Structures)

**å­¦ä¹ ç›®æ ‡**ï¼šæ­å»ºé¡¹ç›®éª¨æ¶ï¼Œå®šä¹‰æ ¸å¿ƒæ•°æ®ç»“æ„ï¼Œç†è§£ Sequence æŠ½è±¡

#### ä¸Šåˆ (AM) - ç¯å¢ƒå‡†å¤‡ä¸é¡¹ç›®åˆå§‹åŒ–

- [âœ…ï¸] **1.1** å…‹éš†å‚è€ƒä»“åº“åˆ°æœ¬åœ°ï¼Œé€šè¯» README å’Œé¡¹ç›®ç»“æ„
- [âœ…ï¸] **1.2** æ­å»ºå¼€å‘ç¯å¢ƒï¼ˆPython 3.10+, PyTorch 2.0+, CUDAï¼‰
- [âœ…ï¸] **1.3** å®‰è£…ä¾èµ–ï¼š`flash-attn`, `triton`, `transformers`
- [âœ…ï¸] **1.4** åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„ï¼ˆæŒ‰ä¸Šè¿°ç›®å½•æ ‘ï¼‰
- [âœ…ï¸] **1.5** é˜…è¯»å‚è€ƒä»“åº“çš„ `config.py`ï¼Œç†è§£é…ç½®é¡¹å«ä¹‰
- [âœ…ï¸] **1.6** é˜…è¯»å‚è€ƒä»“åº“çš„ `sampling_params.py`ï¼Œç†è§£é‡‡æ ·å‚æ•°

#### ä¸‹åˆ (PM) - æ ¸å¿ƒæ•°æ®ç»“æ„å®ç°

- [âœ…ï¸] **1.7** æ‰‹å†™ `nanovllm/config.py`
  - [âœ…ï¸] å®šä¹‰ `Config` ç±»
  - [âœ…ï¸] å¤„ç†æ¨¡å‹è·¯å¾„ã€å¹¶å‘å‚æ•°ã€dtype ç­‰é…ç½®
- [âœ…ï¸] **1.8** æ‰‹å†™ `nanovllm/sampling_params.py`
  - [âœ…ï¸] å®šä¹‰ `SamplingParams` æ•°æ®ç±»
  - [âœ…ï¸] åŒ…å« temperature, top_k, top_p, max_tokens ç­‰å‚æ•°
- [âœ…ï¸] **1.9** æ‰‹å†™ `nanovllm/engine/sequence.py`
  - [âœ…ï¸] å®šä¹‰ `SequenceStatus` æšä¸¾ï¼ˆWaiting/Running/Finishedï¼‰
  - [âœ…ï¸] å®šä¹‰ `Sequence` ç±»
  - [âœ…ï¸] å®ç° token_ids ç®¡ç†
  - [âœ…ï¸] ç†è§£å¹¶é¢„ç•™ `block_table` å±æ€§ï¼ˆPagedAttention ä¼ç¬”ï¼‰
- [âœ…ï¸] **1.10** æ‰‹å†™ `nanovllm/utils/context.py`
  - [âœ…ï¸] å®ç°å…¨å±€ä¸Šä¸‹æ–‡ç®¡ç†å™¨
  - [âœ…ï¸] ç”¨äºè·¨æ¨¡å—ä¼ é€’å…ƒæ•°æ®

#### Day 1 æ£€æŸ¥ç‚¹ âœ…

- [âœ…ï¸] èƒ½å¤Ÿåˆ›å»º `Config` å®ä¾‹å¹¶æ‰“å°é…ç½®
- [âœ…ï¸] èƒ½å¤Ÿåˆ›å»º `Sequence` å®ä¾‹å¹¶ç®¡ç†çŠ¶æ€
- [âœ…ï¸] ç†è§£ Request â†’ Sequence å°è£…çš„è®¾è®¡æ„å›¾
- [âœ…ï¸] ç†è§£ `block_table` åœ¨åç»­ PagedAttention ä¸­çš„ä½œç”¨

---

### Day 2: æ¨¡å‹æ¶æ„æ­å»º (Model Architecture)

**å­¦ä¹ ç›®æ ‡**ï¼šå®ç° Transformer æ ¸å¿ƒç»„ä»¶ï¼Œæ­å»º Qwen3/Llama æ¨¡å‹éª¨æ¶

#### ä¸Šåˆ (AM) - åŸºç¡€å±‚å®ç°

- [ ] **2.1** é˜…è¯» RMSNorm è®ºæ–‡/åšå®¢ï¼Œç†è§£ä¸ LayerNorm çš„åŒºåˆ«
- [ ] **2.2** æ‰‹å†™ `nanovllm/layers/layernorm.py`
  - [ ] å®ç° `RMSNorm` ç±»
  - [ ] æ³¨æ„ eps å‚æ•°å’Œæƒé‡åˆå§‹åŒ–
- [ ] **2.3** é˜…è¯» SiLU æ¿€æ´»å‡½æ•°åŸç†
- [ ] **2.4** æ‰‹å†™ `nanovllm/layers/activation.py`
  - [ ] å®ç° `SiluAndMul` ç±»ï¼ˆGLU å˜ä½“ï¼‰
- [ ] **2.5** æ·±å…¥é˜…è¯» RoPE è®ºæ–‡ï¼Œç†è§£æ—‹è½¬ä½ç½®ç¼–ç æ•°å­¦åŸç†
- [ ] **2.6** æ‰‹å†™ `nanovllm/layers/rotary_embedding.py`
  - [ ] å®ç°é¢‘ç‡è®¡ç®— (freqs)
  - [ ] å®ç° apply_rotary_emb å‡½æ•°

#### ä¸‹åˆ (PM) - æ¨¡å‹éª¨æ¶æ­å»º

- [ ] **2.7** é˜…è¯» Qwen3/Llama æ¨¡å‹ç»“æ„ï¼Œç†è§£å„å±‚ç»„æˆ
- [ ] **2.8** æ‰‹å†™ `nanovllm/models/qwen3.py`ï¼ˆç®€åŒ–ç‰ˆï¼‰
  - [ ] å®ç° `Qwen3Attention` ç±»ï¼ˆå…ˆç”¨æ™®é€š nn.Linearï¼‰
  - [ ] å®ç° `Qwen3MLP` ç±»
  - [ ] å®ç° `Qwen3DecoderLayer` ç±»
  - [ ] å®ç° `Qwen3Model` ç±»ï¼ˆEmbedding + Layers + Normï¼‰
  - [ ] å®ç° `Qwen3ForCausalLM` ç±»ï¼ˆåŠ  lm_headï¼‰
- [ ] **2.9** ç¼–å†™ç®€å•æµ‹è¯•ï¼šéšæœºè¾“å…¥èƒ½å¦é€šè¿‡ forward

#### Day 2 æ£€æŸ¥ç‚¹ âœ…

- [ ] RMSNorm å•å…ƒæµ‹è¯•é€šè¿‡
- [ ] RoPE ä½ç½®ç¼–ç è®¡ç®—æ­£ç¡®
- [ ] æ¨¡å‹èƒ½å®Œæˆä¸€æ¬¡ forward passï¼ˆéšæœºæƒé‡ï¼‰
- [ ] èƒ½æ¸…æ™°è§£é‡Š RoPE çš„æ•°å­¦åŸç†

---

### Day 3: æ ¸å¿ƒçµé­‚ - æ˜¾å­˜ç®¡ç†ä¸ PagedAttention (Memory Management)

**å­¦ä¹ ç›®æ ‡**ï¼šç†è§£å¹¶å®ç° vLLM æœ€æ ¸å¿ƒçš„ KV Cache åˆ†é¡µç®¡ç†

#### ä¸Šåˆ (AM) - Block ç®¡ç†å™¨

- [ ] **3.1** ç²¾è¯» vLLM PagedAttention è®ºæ–‡ï¼ˆé‡ç‚¹ç¬¬3èŠ‚ï¼‰
- [ ] **3.2** ç†è§£ç‰©ç†å— vs é€»è¾‘å—çš„æ¦‚å¿µ
- [ ] **3.3** ç†è§£ block_table æ˜ å°„æœºåˆ¶
- [ ] **3.4** æ‰‹å†™ `nanovllm/engine/block_manager.py`
  - [ ] å®šä¹‰ `Block` ç±»
    - [ ] åŒ…å« block_id, ref_count
    - [ ] å®ç° hash è®¡ç®—ï¼ˆPrefix Caching ç”¨ï¼‰
  - [ ] å®šä¹‰ `BlockManager` ç±»
    - [ ] åˆå§‹åŒ– free_blocks æ± 
    - [ ] å®ç° `allocate()` æ–¹æ³• - ä¸ºæ–°åºåˆ—åˆ†é…å—
    - [ ] å®ç° `append_slot()` æ–¹æ³• - è¿½åŠ  token æ—¶çš„å—ç®¡ç†

#### ä¸‹åˆ (PM) - Attention å±‚ä¸ KV Cache

- [ ] **3.5** é˜…è¯» FlashAttention è®ºæ–‡ï¼Œç†è§£å…¶ä¼˜åŒ–åŸç†
- [ ] **3.6** å­¦ä¹  flash_attn åº“ API
- [ ] **3.7** æ‰‹å†™ `nanovllm/layers/attention.py`
  - [ ] é›†æˆ `flash_attn` åº“
  - [ ] å®ç° `Attention` ç±»
  - [ ] å®ç° KV Cache çš„è¯»å†™é€»è¾‘
  - [ ] ç¼–å†™ `store_kvcache` å‡½æ•°ï¼ˆTriton/PyTorchï¼‰
- [ ] **3.8** ç†è§£ Prefill vs Decode é˜¶æ®µçš„ Attention å·®å¼‚
- [ ] **3.9** ç”»å›¾ï¼šç‰©ç†å—ã€é€»è¾‘å—ã€block_table çš„å…³ç³»

#### Day 3 æ£€æŸ¥ç‚¹ âœ…

- [ ] èƒ½æ¸…æ™°è§£é‡Š PagedAttention è§£å†³äº†ä»€ä¹ˆé—®é¢˜
- [ ] BlockManager èƒ½æ­£ç¡®åˆ†é…å’Œé‡Šæ”¾å—
- [ ] Attention å±‚èƒ½æ­£ç¡®è¯»å†™ KV Cache
- [ ] ç†è§£ hash åœ¨ Prefix Caching ä¸­çš„ä½œç”¨

---

### Day 4: è°ƒåº¦å™¨ä¸æ‰§è¡Œå¼•æ“ (Scheduler & Execution)

**å­¦ä¹ ç›®æ ‡**ï¼šå®ç° Continuous Batching è°ƒåº¦é€»è¾‘

#### ä¸Šåˆ (AM) - è°ƒåº¦å™¨å®ç°

- [ ] **4.1** é˜…è¯» vLLM è®ºæ–‡ä¸­çš„è°ƒåº¦ç­–ç•¥éƒ¨åˆ†
- [ ] **4.2** ç†è§£ Continuous Batching vs Static Batching
- [ ] **4.3** æ‰‹å†™ `nanovllm/engine/scheduler.py`
  - [ ] ç»´æŠ¤ `waiting` é˜Ÿåˆ—
  - [ ] ç»´æŠ¤ `running` é˜Ÿåˆ—
  - [ ] å®ç° `add_sequence()` æ–¹æ³•
  - [ ] å®ç° `schedule()` æ–¹æ³•
    - [ ] æ£€æŸ¥æ˜¾å­˜æ˜¯å¦è¶³å¤Ÿ
    - [ ] å°† waiting åºåˆ—ç§»åˆ° running
    - [ ] è°ƒç”¨ BlockManager åˆ†é…å—
  - [ ] å®ç° `postprocess()` æ–¹æ³•
    - [ ] å¤„ç†å·²å®Œæˆçš„åºåˆ—
    - [ ] é‡Šæ”¾å¯¹åº”çš„å—

#### ä¸‹åˆ (PM) - ModelRunner åŸºç¡€ç‰ˆ

- [ ] **4.4** æ‰‹å†™ `nanovllm/engine/model_runner.py`ï¼ˆåŸºç¡€ç‰ˆï¼‰
  - [ ] å®ç° `__init__` - åŠ è½½æ¨¡å‹å’Œ tokenizer
  - [ ] å®ç° `allocate_kv_cache()` - é¢„åˆ†é… GPU æ˜¾å­˜
  - [ ] å®ç° `prepare_input()` æ–¹æ³•
    - [ ] å°†å¤šä¸ª Sequence çš„ token æ‹¼æˆ batch
    - [ ] ç”Ÿæˆ attention_mask
    - [ ] ç”Ÿæˆ position_ids
    - [ ] æ„å»º block_tables tensor
  - [ ] å®ç°åŸºç¡€ `run()` æ–¹æ³• - æ‰§è¡Œ forward
- [ ] **4.5** ç†è§£è°ƒåº¦å™¨å¦‚ä½•ä¸ BlockManager äº¤äº’

#### Day 4 æ£€æŸ¥ç‚¹ âœ…

- [ ] è°ƒåº¦å™¨èƒ½æ­£ç¡®ç®¡ç† waiting/running é˜Ÿåˆ—
- [ ] èƒ½æ ¹æ®æ˜¾å­˜æƒ…å†µåšå‡ºè°ƒåº¦å†³ç­–
- [ ] ModelRunner èƒ½æ­£ç¡®å‡†å¤‡ batch è¾“å…¥
- [ ] ç†è§£ Continuous Batching çš„ä¼˜åŠ¿

---

### Day 5: å®Œæ•´æ¨ç†å¾ªç¯ä¸é‡‡æ · (Inference Loop & Sampler)

**å­¦ä¹ ç›®æ ‡**ï¼šä¸²è”æ‰€æœ‰ç»„ä»¶ï¼Œå®ç°å®Œæ•´ generate æµç¨‹

#### ä¸Šåˆ (AM) - Sampler å®ç°

- [ ] **5.1** å¤ä¹ é‡‡æ ·ç®—æ³•ï¼šGreedy, Temperature, Top-K, Top-P
- [ ] **5.2** æ‰‹å†™ `nanovllm/layers/sampler.py`
  - [ ] å®ç°æ¸©åº¦ç¼©æ”¾
  - [ ] å®ç° Top-K è¿‡æ»¤
  - [ ] å®ç° Top-P (Nucleus) è¿‡æ»¤
  - [ ] å®ç°æœ€ç»ˆé‡‡æ ·é€»è¾‘
  - [ ] å®ç° `Sampler` ç±»æ•´åˆä»¥ä¸ŠåŠŸèƒ½
- [ ] **5.3** ç¼–å†™ Sampler å•å…ƒæµ‹è¯•

#### ä¸‹åˆ (PM) - LLMEngine ä¸æ¨ç†å¾ªç¯

- [ ] **5.4** æ‰‹å†™ `nanovllm/engine/llm_engine.py`
  - [ ] å®ç° `__init__` - åˆå§‹åŒ–å„ç»„ä»¶
  - [ ] å®ç° `add_request()` - æ·»åŠ æ¨ç†è¯·æ±‚
  - [ ] å®ç° `step()` å‡½æ•°
    - [ ] è°ƒç”¨ `scheduler.schedule()`
    - [ ] åŒºåˆ† Prefill å’Œ Decode é˜¶æ®µ
    - [ ] è°ƒç”¨ `model_runner.run()`
    - [ ] è°ƒç”¨ `sampler` é‡‡æ ·
    - [ ] è°ƒç”¨ `scheduler.postprocess()`
  - [ ] å®ç° `generate()` - å®Œæ•´ç”Ÿæˆå¾ªç¯
- [ ] **5.5** æ‰‹å†™ `nanovllm/llm.py`
  - [ ] å®ç°ç”¨æˆ·ä¾§ API
  - [ ] å°è£… LLMEngine
- [ ] **5.6** ç¼–å†™ `example.py` æµ‹è¯•è„šæœ¬
- [ ] **5.7** ğŸ‰ **é‡Œç¨‹ç¢‘**ï¼šè·‘é€šå•å¡æ¨ç† demoï¼

#### Day 5 æ£€æŸ¥ç‚¹ âœ…

- [ ] Sampler é‡‡æ ·ç»“æœç¬¦åˆé¢„æœŸåˆ†å¸ƒ
- [ ] èƒ½å¤Ÿå®Œæˆä¸€ä¸ªå®Œæ•´çš„æ–‡æœ¬ç”Ÿæˆ
- [ ] Prefill å’Œ Decode é˜¶æ®µæ­£ç¡®åŒºåˆ†
- [ ] example.py èƒ½æ­£å¸¸è¿è¡Œå¹¶è¾“å‡ºç»“æœ

---

### Day 6: é«˜çº§ç‰¹æ€§ - å¼ é‡å¹¶è¡Œä¸ CUDA Graph (Optimization)

**å­¦ä¹ ç›®æ ‡**ï¼šå®ç°å¤šå¡å¹¶è¡Œå’Œ CUDA Graph ä¼˜åŒ–

#### ä¸Šåˆ (AM) - Tensor Parallelism

- [ ] **6.1** å­¦ä¹  Tensor Parallelism åŸç†ï¼ˆMegatron-LM è®ºæ–‡ï¼‰
- [ ] **6.2** ç†è§£ ColumnParallel vs RowParallel çš„åŒºåˆ«
- [ ] **6.3** å­¦ä¹  `torch.distributed` APIï¼ˆinit_process_group, all_reduceï¼‰
- [ ] **6.4** æ‰‹å†™ `nanovllm/layers/linear.py`
  - [ ] å®ç° `ColumnParallelLinear` ç±»
    - [ ] æƒé‡æŒ‰åˆ—åˆ‡åˆ†
    - [ ] forward åæ— éœ€ all_reduce
  - [ ] å®ç° `RowParallelLinear` ç±»
    - [ ] æƒé‡æŒ‰è¡Œåˆ‡åˆ†
    - [ ] forward åéœ€è¦ all_reduce
- [ ] **6.5** ä¿®æ”¹ `qwen3.py`ï¼Œæ›¿æ¢ä¸ºå¹¶è¡Œ Linear å±‚

#### ä¸‹åˆ (PM) - CUDA Graph ä¼˜åŒ–

- [ ] **6.6** å­¦ä¹  CUDA Graph åŸç†å’Œä½¿ç”¨åœºæ™¯
- [ ] **6.7** ç†è§£ä¸ºä»€ä¹ˆ CUDA Graph å¯¹ Decode é˜¶æ®µæœ‰æ•ˆ
- [ ] **6.8** ä¿®æ”¹ `nanovllm/engine/model_runner.py`ï¼ˆè¿›é˜¶ç‰ˆï¼‰
  - [ ] æ·»åŠ å¤šè¿›ç¨‹åˆå§‹åŒ–ä»£ç 
  - [ ] å®ç° `capture_cudagraph()` æ–¹æ³•
    - [ ] å½•åˆ¶è®¡ç®—å›¾
    - [ ] å¤„ç†é™æ€ shape è¦æ±‚
  - [ ] å®ç° `replay()` æ–¹æ³•
    - [ ] é‡æ”¾å½•åˆ¶çš„è®¡ç®—å›¾
  - [ ] åœ¨ Decode é˜¶æ®µä½¿ç”¨ CUDA Graph
- [ ] **6.9** æµ‹è¯•å¤šå¡è¿è¡Œï¼ˆå¦‚æœæœ‰å¤šå¡ï¼‰

#### Day 6 æ£€æŸ¥ç‚¹ âœ…

- [ ] ColumnParallel å’Œ RowParallel æ­£ç¡®åˆ‡åˆ†æƒé‡
- [ ] all_reduce é€šä¿¡æ­£ç¡®
- [ ] CUDA Graph å½•åˆ¶å’Œé‡æ”¾æ­£å¸¸å·¥ä½œ
- [ ] èƒ½æ¸…æ™°è§£é‡Š TP çš„é€šä¿¡æ¨¡å¼

---

### Day 7: æµ‹è¯•ã€Benchmark ä¸ç®€å†æ‰“ç£¨ (Final Polish)

**å­¦ä¹ ç›®æ ‡**ï¼šéªŒè¯æ€§èƒ½ï¼Œæ•´ç†æ–‡æ¡£ï¼Œè½¬åŒ–ä¸ºç®€å†è¯­è¨€

#### ä¸Šåˆ (AM) - æ€§èƒ½æµ‹è¯•ä¸è°ƒè¯•

- [ ] **7.1** è¿è¡Œ `bench.py` è¿›è¡Œæ€§èƒ½æµ‹è¯•
  - [ ] æµ‹é‡ååé‡ï¼ˆTokens/sï¼‰
  - [ ] æµ‹é‡é¦– token å»¶è¿Ÿï¼ˆTime to First Tokenï¼‰
  - [ ] æµ‹é‡ç”Ÿæˆå»¶è¿Ÿï¼ˆTime per Output Tokenï¼‰
- [ ] **7.2** ä¸ HuggingFace åŸç”Ÿå®ç°å¯¹æ¯”æ€§èƒ½
- [ ] **7.3** æ£€æŸ¥å†…å­˜æ³„æ¼
  - [ ] ä½¿ç”¨ `torch.cuda.memory_stats()`
  - [ ] é•¿æ—¶é—´è¿è¡Œæµ‹è¯•
- [ ] **7.4** ä¿®å¤å‘ç°çš„ bug

#### ä¸‹åˆ (PM) - ä»£ç å¤ç›˜ä¸ç®€å†æ•´ç†

- [ ] **7.5** Code Reviewï¼šé€šè¯»æ‰€æœ‰ä»£ç 
  - [ ] é‡ç‚¹å¤ä¹  `block_manager.py`
  - [ ] é‡ç‚¹å¤ä¹  `attention.py`
  - [ ] é‡ç‚¹å¤ä¹  `scheduler.py`
  - [ ] ç¡®ä¿ç†è§£æ¯ä¸€è¡Œä»£ç 
- [ ] **7.6** æ•´ç†é¡¹ç›®æ–‡æ¡£
  - [ ] ç¼–å†™ README.md
  - [ ] æ·»åŠ æ¶æ„å›¾
  - [ ] è®°å½•æ€§èƒ½æ•°æ®
- [ ] **7.7** å‡†å¤‡ç®€å†æè¿°ï¼ˆè§ä¸‹æ–¹æ¨¡æ¿ï¼‰
- [ ] **7.8** å‡†å¤‡é¢è¯•å¯èƒ½è¢«é—®åˆ°çš„é—®é¢˜

#### Day 7 æ£€æŸ¥ç‚¹ âœ…

- [ ] æ€§èƒ½æ•°æ®è®°å½•å®Œæ•´
- [ ] ä»£ç æ— æ˜æ˜¾ bug
- [ ] èƒ½æµç•…è§£é‡Šä»»æ„æ¨¡å—çš„å®ç°
- [ ] ç®€å†æè¿°å‡†å¤‡å®Œæˆ

---

## ğŸ“ ç®€å†äº®ç‚¹æ¨¡æ¿

### é¡¹ç›®åç§°

**nano-vLLMï¼šé«˜æ€§èƒ½ LLM æ¨ç†å¼•æ“å¤ç°**

### é¡¹ç›®æè¿°

ä»é›¶å®ç°äº†ç±» vLLM çš„é«˜æ€§èƒ½å¤§æ¨¡å‹æ¨ç†å¼•æ“ï¼Œæ”¯æŒ Qwen3/Llama ç³»åˆ—æ¨¡å‹ã€‚

### æŠ€æœ¯äº®ç‚¹ï¼ˆæ ¹æ®å®é™…å®Œæˆæƒ…å†µé€‰æ‹©ï¼‰

- [ ] å®ç°äº† **PagedAttention** å†…å­˜ç®¡ç†æœºåˆ¶ï¼Œé€šè¿‡åˆ†é¡µç®¡ç† KV Cache è§£å†³æ˜¾å­˜ç¢ç‰‡åŒ–é—®é¢˜
- [ ] å®ç°äº† **Continuous Batching** è°ƒåº¦ç­–ç•¥ï¼Œç›¸æ¯”é™æ€ Batching æå‡ X% ååé‡
- [ ] é›†æˆ **FlashAttention**ï¼Œä¼˜åŒ– Attention è®¡ç®—æ€§èƒ½
- [ ] å®ç°äº†åŸºäº **NCCL çš„ Tensor Parallelism**ï¼Œæ”¯æŒå¤šå¡æ¨ç†
- [ ] ä½¿ç”¨ **CUDA Graph** ä¼˜åŒ– Decode é˜¶æ®µï¼Œå‡å°‘ Kernel Launch å¼€é”€
- [ ] æ‰‹å†™å®ç° **RoPE**ã€**RMSNorm** ç­‰ Transformer æ ¸å¿ƒç»„ä»¶

### æ€§èƒ½æ•°æ®ï¼ˆå¾…å¡«å†™ï¼‰

| æŒ‡æ ‡ | æœ¬é¡¹ç›® | HuggingFace | æå‡ |
|------|--------|-------------|------|
| ååé‡ (tokens/s) | - | - | - |
| é¦– Token å»¶è¿Ÿ | - | - | - |
| æ˜¾å­˜å ç”¨ | - | - | - |

---

## â“ é¢è¯•é«˜é¢‘é—®é¢˜å‡†å¤‡

### PagedAttention ç›¸å…³

- [ ] Q: ä»€ä¹ˆæ˜¯ PagedAttentionï¼Ÿè§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ
- [ ] Q: ç‰©ç†å—å’Œé€»è¾‘å—æ˜¯å¦‚ä½•æ˜ å°„çš„ï¼Ÿ
- [ ] Q: Prefix Caching æ˜¯å¦‚ä½•å®ç°çš„ï¼Ÿ

### è°ƒåº¦ç›¸å…³

- [ ] Q: Continuous Batching å’Œ Static Batching æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
- [ ] Q: è°ƒåº¦å™¨æ˜¯å¦‚ä½•å†³å®šå“ªäº›åºåˆ—å¯ä»¥è¿è¡Œçš„ï¼Ÿ

### å¹¶è¡Œç›¸å…³

- [ ] Q: ColumnParallel å’Œ RowParallel çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ
- [ ] Q: ä¸ºä»€ä¹ˆ RowParallel åéœ€è¦ all_reduceï¼Ÿ

### ä¼˜åŒ–ç›¸å…³

- [ ] Q: CUDA Graph çš„åŸç†æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆå¯¹ Decode æœ‰æ•ˆï¼Ÿ
- [ ] Q: FlashAttention æ˜¯å¦‚ä½•ä¼˜åŒ–çš„ï¼Ÿ

---

## ğŸ“Š è¿›åº¦è¿½è¸ª

| Day | ä¸Šåˆ | ä¸‹åˆ | çŠ¶æ€ |
|-----|------|------|------|
| Day 1 | ç¯å¢ƒ & é¡¹ç›®åˆå§‹åŒ– | æ ¸å¿ƒæ•°æ®ç»“æ„ | â¬œ æœªå¼€å§‹ |
| Day 2 | åŸºç¡€å±‚ (Norm/Activation/RoPE) | æ¨¡å‹éª¨æ¶ | â¬œ æœªå¼€å§‹ |
| Day 3 | Block ç®¡ç†å™¨ | Attention & KV Cache | â¬œ æœªå¼€å§‹ |
| Day 4 | è°ƒåº¦å™¨ | ModelRunner | â¬œ æœªå¼€å§‹ |
| Day 5 | Sampler | LLMEngine & å®Œæ•´æµç¨‹ | â¬œ æœªå¼€å§‹ |
| Day 6 | Tensor Parallelism | CUDA Graph | â¬œ æœªå¼€å§‹ |
| Day 7 | æ€§èƒ½æµ‹è¯• | ä»£ç å¤ç›˜ & ç®€å† | â¬œ æœªå¼€å§‹ |

**å›¾ä¾‹**ï¼šâ¬œ æœªå¼€å§‹ | ğŸŸ¡ è¿›è¡Œä¸­ | âœ… å·²å®Œæˆ

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [vLLM è®ºæ–‡](https://arxiv.org/abs/2309.06180)
- [FlashAttention è®ºæ–‡](https://arxiv.org/abs/2205.14135)
- [RoPE è®ºæ–‡](https://arxiv.org/abs/2104.09864)
- [Megatron-LM è®ºæ–‡](https://arxiv.org/abs/1909.08053)
- [nano-vllm æºç ](https://github.com/GeeeekExplorer/nano-vllm)

---

*æœ€åæ›´æ–°: 2026å¹´1æœˆ2æ—¥*
