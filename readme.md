## ğŸ“‹ é¡¹ç›®æ¦‚è§ˆ

- **é¡¹ç›®ç›®æ ‡**ï¼šä»é›¶å¤ç° nano-vLLMï¼Œæ·±å…¥ç†è§£ vLLM æ ¸å¿ƒæ¶æ„
- **å‚è€ƒä»“åº“**ï¼š<https://github.com/GeeeekExplorer/nano-vllm.git>
- **æ—¶é—´å‘¨æœŸ**ï¼š7 å¤©ï¼ˆ2026å¹´1æœˆ2æ—¥ - 2026å¹´1æœˆ8æ—¥ï¼‰
- **æ ¸å¿ƒæŠ€æœ¯ç‚¹**ï¼šPagedAttentionã€KV Cache ç®¡ç†ã€FlashAttentionã€CUDA Graphã€Tensor Parallelism

---

## âœ¨ é¡¹ç›®ç‰¹ç‚¹

- **PagedAttention**ï¼šé€šè¿‡åˆ†é¡µç®¡ç† KV Cacheï¼Œè§£å†³æ˜¾å­˜ç¢ç‰‡åŒ–é—®é¢˜ã€‚
- **é«˜æ€§èƒ½æ¨ç†**ï¼šé›†æˆ FlashAttention å’Œ CUDA Graphï¼Œä¼˜åŒ–æ¨ç†æ€§èƒ½ã€‚
- **åˆ†å¸ƒå¼æ”¯æŒ**ï¼šåŸºäº NCCL çš„ Tensor Parallelismï¼Œæ”¯æŒå¤šå¡æ¨ç†ã€‚
- **æ¨¡å—åŒ–è®¾è®¡**ï¼šä»é…ç½®ã€æ•°æ®ç»“æ„åˆ°æ¨ç†å¼•æ“ï¼Œæ¨¡å—åŒ–å®ç°ï¼Œä¾¿äºæ‰©å±•ã€‚

---

## ğŸ“ é¡¹ç›®ç›®å½•ç»“æ„

```
nanovllm/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ config.py
â”œâ”€â”€ sampling_params.py
â”œâ”€â”€ llm.py
â”œâ”€â”€ engine/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ sequence.py
â”‚   â”œâ”€â”€ block_manager.py
â”‚   â”œâ”€â”€ scheduler.py
â”‚   â”œâ”€â”€ model_runner.py
â”‚   â””â”€â”€ llm_engine.py
â”œâ”€â”€ layers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ layernorm.py
â”‚   â”œâ”€â”€ activation.py
â”‚   â”œâ”€â”€ rotary_embedding.py
â”‚   â”œâ”€â”€ attention.py
â”‚   â”œâ”€â”€ linear.py
â”‚   â””â”€â”€ sampler.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ qwen3.py
â””â”€â”€ utils/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ context.py
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

- **ä¾èµ–ç¯å¢ƒ**ï¼š
  - Python 3.10+
  - PyTorch 2.0+
  - CUDA 11.7+
- **å®‰è£…ä¾èµ–**ï¼š
  ```bash
  pip install flash-attn triton transformers
  ```

### 2. å…‹éš†ä»“åº“

```bash
git clone https://github.com/OnlyHero5/nano_vll_repro.git
cd nano_vll_repro
```

### 3. è¿è¡Œç¤ºä¾‹

è¿è¡Œ `example.py` æµ‹è¯•æ¨ç†æµç¨‹ï¼š

```bash
python example.py --model qwen3 --device cuda --max_tokens 128
```

---

## ğŸ› ï¸ å¼€å‘è·¯çº¿å›¾

### Day 1: åŸºç¡€è®¾æ–½ä¸æ•°æ®ç»“æ„

- [x] åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„
- [x] å®ç° `Config` å’Œ `SamplingParams` æ•°æ®ç±»
- [x] å®ç° `Sequence` æ•°æ®ç»“æ„ï¼Œæ”¯æŒ token ç®¡ç†

### Day 2: æ¨¡å‹æ¶æ„æ­å»º

- [x] å®ç° Transformer æ ¸å¿ƒç»„ä»¶ï¼ˆRMSNormã€RoPEã€Attention ç­‰ï¼‰
- [x] æ­å»º Qwen3 æ¨¡å‹éª¨æ¶ï¼Œæ”¯æŒéšæœºè¾“å…¥çš„ forward pass

### Day 3: æ˜¾å­˜ç®¡ç†ä¸ PagedAttention

- [x] å®ç° `BlockManager`ï¼Œæ”¯æŒ KV Cache åˆ†é¡µç®¡ç†
- [x] é›†æˆ FlashAttentionï¼Œä¼˜åŒ– Attention æ€§èƒ½

### Day 4: è°ƒåº¦å™¨ä¸æ‰§è¡Œå¼•æ“

- [x] å®ç° Continuous Batching è°ƒåº¦å™¨
- [x] å®ç° `ModelRunner`ï¼Œæ”¯æŒæ‰¹é‡æ¨ç†

### Day 5: å®Œæ•´æ¨ç†å¾ªç¯ä¸é‡‡æ ·

- [x] å®ç°é‡‡æ ·å™¨ï¼ˆGreedy, Top-K, Top-P ç­‰ï¼‰
- [x] å®ç°æ¨ç†å¼•æ“ `LLMEngine`ï¼Œæ”¯æŒå®Œæ•´ç”Ÿæˆæµç¨‹

### Day 6: é«˜çº§ç‰¹æ€§ä¼˜åŒ–

- [x] å®ç° Tensor Parallelismï¼Œæ”¯æŒå¤šå¡æ¨ç†
- [x] é›†æˆ CUDA Graphï¼Œä¼˜åŒ– Decode é˜¶æ®µæ€§èƒ½

### Day 7: æµ‹è¯•ä¸æ–‡æ¡£æ•´ç†

- [x] æ€§èƒ½æµ‹è¯•ä¸è°ƒè¯•
- [x] æ•´ç†æ–‡æ¡£ä¸ä»£ç ï¼Œå‡†å¤‡ç®€å†æè¿°

---

## ğŸ“Š æ€§èƒ½æ•°æ®

| æŒ‡æ ‡               | æœ¬é¡¹ç›®         | HuggingFace | æå‡  |
|--------------------|----------------|-------------|-------|
| ååé‡ (tokens/s) | å¾…æµ‹è¯•         | å¾…æµ‹è¯•      | å¾…æµ‹è¯• |
| é¦– Token å»¶è¿Ÿ      | å¾…æµ‹è¯•         | å¾…æµ‹è¯•      | å¾…æµ‹è¯• |
| æ˜¾å­˜å ç”¨           | å¾…æµ‹è¯•         | å¾…æµ‹è¯•      | å¾…æµ‹è¯• |

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [vLLM è®ºæ–‡](https://arxiv.org/abs/2309.06180)
- [FlashAttention è®ºæ–‡](https://arxiv.org/abs/2205.14135)
- [RoPE è®ºæ–‡](https://arxiv.org/abs/2104.09864)
- [Megatron-LM è®ºæ–‡](https://arxiv.org/abs/1909.08053)
- [nano-vllm æºç ](https://github.com/GeeeekExplorer/nano-vllm)

---

## ğŸ“ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ª MIT è®¸å¯è¯ï¼Œè¯¦æƒ…è¯·å‚é˜… [LICENSE](./LICENSE) æ–‡ä»¶ã€‚
```